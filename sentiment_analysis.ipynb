{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj6cbR0JBTCP"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import pickle\n",
        "import tensorflow as tf \n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model,Input\n",
        "from tensorflow.keras.layers import *\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
        "\n",
        "from prediction import predict \n",
        "from model import sentiment_model"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = 'https://raw.githubusercontent.com/malinphy/datasets/main/tweet_sentiment_extraction/twitter_sentiment_analysis/twitter_training.csv'\n",
        "test_path = 'https://raw.githubusercontent.com/malinphy/datasets/main/tweet_sentiment_extraction/twitter_sentiment_analysis/twitter_validation.csv'"
      ],
      "metadata": {
        "id": "baJil6B3N1ir"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(train_path,header = None).dropna().reset_index(drop= True)\n",
        "test_df = pd.read_csv(test_path,header = None).reset_index(drop= True)\n",
        "train_df = train_df.rename(columns={0: 'tweet_id', 1: 'entity',2:'sentiment',3:'content'})\n",
        "test_df = test_df.rename(columns={0: 'tweet_id', 1: 'entity',2:'sentiment',3:'content'})"
      ],
      "metadata": {
        "id": "NjqgsiTfN1fC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 45000\n",
        "embed_dim = 32\n",
        "input_len = 170"
      ],
      "metadata": {
        "id": "DUp5IBwCN1bw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=170\n",
        "    )\n",
        "\n",
        "vectorize_layer.adapt(train_df['content'])\n",
        "train_tokens = vectorize_layer(train_df['content'])\n",
        "test_tokens = vectorize_layer(test_df['content'])"
      ],
      "metadata": {
        "id": "4Mbsi__gN1YN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_len = tf.shape(train_tokens)[1]\n",
        "corpus_size = len(vectorize_layer.get_vocabulary())"
      ],
      "metadata": {
        "id": "5eIGwbhMN1So"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3HBcvimuAtW",
        "outputId": "c9d37f6e-8e36-4867-fbfe-b65a053f2ddd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int32, numpy=170>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LE = LabelEncoder()\n",
        "train_encoded_labels = LE.fit_transform(train_df['sentiment'])\n",
        "test_encoded_labels = LE.transform(test_df['sentiment'])"
      ],
      "metadata": {
        "id": "LhqFLtUHN1PD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_model():\n",
        "    input_len = 170\n",
        "    vocab_size = 45000\n",
        "    embed_dim = 32 \n",
        "    input_layer = Input(shape=(input_len,), name = 'input_layer')\n",
        "    emb_layer = Embedding(vocab_size, embed_dim, name = 'embedding_layer')(input_layer)\n",
        "    flat_layer = Flatten(name = 'Flatten_layer')(emb_layer)\n",
        "    d1_layer = Dense(128,activation = 'relu',name = 'd1_layer')(flat_layer)\n",
        "    d2_layer = Dense(64,activation = 'relu',name = 'd2_layer')(d1_layer)\n",
        "    d3_layer = Dense(32,activation = 'relu',name = 'd3_layer')(d2_layer)\n",
        "    final_layer = Dense(4,activation = 'softmax',name = 'final_layer')(d3_layer)\n",
        "\n",
        "    return Model(inputs = input_layer, outputs = final_layer)\n",
        "\n",
        "model = sentiment_model()\n",
        "\n",
        "model.compile(\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    optimizer = tf.keras.optimizers.Adam(),\n",
        "    metrics = ['accuracy']\n",
        "    )\n",
        "\n",
        "# history = model.fit(\n",
        "#     train_tokens,\n",
        "#     train_encoded_labels,\n",
        "#     epochs = 10,\n",
        "#     # validation_split = 0.2\n",
        "#     )\n",
        "\n",
        "# model.save_weights('sentiment_model.h5')"
      ],
      "metadata": {
        "id": "0WWH1PW_N1Ab"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('sentiment_model.h5')"
      ],
      "metadata": {
        "id": "BCbDJHNCTNy6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = tf.math.top_k(model.predict(test_tokens), k=1 )[1]"
      ],
      "metadata": {
        "id": "4o159kcKN08l"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(test_encoded_labels, preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-4cpamcN05K",
        "outputId": "c48be4f3-923f-4613-9baa-1d77d09f501a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[168,   2,   1,   1],\n",
              "       [  2, 263,   0,   1],\n",
              "       [  3,   3, 275,   4],\n",
              "       [  1,   4,   3, 269]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('F1 SCORE',f1_score(test_encoded_labels, preds, average= 'micro'))\n",
        "print(classification_report(test_encoded_labels, preds,  target_names = list(LE.inverse_transform([0,1,2,3]))  ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2rTSjbJN01p",
        "outputId": "21a7c1d8-7b31-4034-aade-bc71bde5a3cf"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 SCORE 0.975\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Irrelevant       0.97      0.98      0.97       172\n",
            "    Negative       0.97      0.99      0.98       266\n",
            "     Neutral       0.99      0.96      0.98       285\n",
            "    Positive       0.98      0.97      0.97       277\n",
            "\n",
            "    accuracy                           0.97      1000\n",
            "   macro avg       0.97      0.98      0.97      1000\n",
            "weighted avg       0.98      0.97      0.97      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump({'config': vectorize_layer.get_config(),\n",
        "             'weights': vectorize_layer.get_weights()}\n",
        "            , open(\"tv_layer.pkl\", \"wb\"))\n",
        "\n",
        "\n",
        "from_disk = pickle.load(open(\"tv_layer.pkl\", \"rb\"))\n",
        "new_v = TextVectorization.from_config(from_disk['config'])\n",
        "# You have to call `adapt` with some dummy data (BUG in Keras)\n",
        "new_v.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
        "new_v.set_weights(from_disk['weights'])\n",
        "\n",
        "output = open('LE.pkl', 'wb')\n",
        "pickle.dump(LE, output)\n",
        "output.close()\n",
        "\n",
        "pkl_file = open('LE.pkl', 'rb')\n",
        "le_departure = pickle.load(pkl_file) \n",
        "pkl_file.close()\n"
      ],
      "metadata": {
        "id": "z3ecYg5-Xzgs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_test = (new_v('I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tomâ€™s great auntie as â€˜Hayley canâ€™t get out of bedâ€™ and told to his grandma, who now thinks Iâ€™m a lazy, terrible person ðŸ¤£'))"
      ],
      "metadata": {
        "id": "ZOEuV_lqaUfm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(x):\n",
        "    from_disk = pickle.load(open(\"tv_layer.pkl\", \"rb\"))\n",
        "    new_v = TextVectorization.from_config(from_disk['config'])\n",
        "    # You have to call `adapt` with some dummy data (BUG in Keras)\n",
        "    new_v.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
        "    new_v.set_weights(from_disk['weights'])\n",
        "    \n",
        "    pkl_file = open('LE.pkl', 'rb')\n",
        "    le_departure = pickle.load(pkl_file) \n",
        "    pkl_file.close()\n",
        "\n",
        "    # y = new_v(x)\n",
        "    model.load_weights('sentiment_model.h5')\n",
        "    test_sent = new_v(x)\n",
        "    test_sent = tf.reshape(test_sent, shape = (1, input_len))\n",
        "    y = np.argmax(model.predict(test_sent))\n",
        "\n",
        "    return (le_departure.inverse_transform([y]))[0]"
      ],
      "metadata": {
        "id": "9yoSya1NsGg3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict('I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tomâ€™s great auntie as â€˜Hayley canâ€™t get out of bedâ€™ and told to his grandma, who now thinks Iâ€™m a lazy, terrible person ðŸ¤£')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Yrv9LVvNXYzA",
        "outputId": "24b24b00-1d7e-45a6-8e0a-26fb58c8d088"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Irrelevant'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h-ixBpWSXf0X"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}